from sklearn import svm
import os
import torch.utils.data
from torch.autograd import Variable
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.nn as nn
import dcgan
from arg_parser import opt
import arg_parser
import numpy as np
from sklearn.externals import joblib

transform = transforms.Compose([
                               transforms.Resize(opt.imageSize),
                               transforms.CenterCrop(opt.imageSize),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                               ])



if __name__ == '__main__':

    # Output file for the features generated by the generator of the ground DCGAN
    file = 'data/disc_features/50_features_food_101'

    if not opt.svm :

        print('------- Save the features generated by the Generator -------')
        dataset = dset.ImageFolder(root=opt.dataroot,
                                   transform=transform)

        dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,
                                                 shuffle=True, num_workers=int(opt.workers))
        device = torch.device("cuda:" + str(opt.gpu) if opt.cuda else "cpu")
        
        # Handle multi-gpu if desired
        netD = dcgan.Generator(opt.ngpu).to(device)
        #if (device.type == 'cuda') and (opt.ngpu > 1):
        netD = nn.DataParallel(netD, list(range(opt.ngpu)))

        def weights_init(m):
            classname = m.__class__.__name__
            if classname.find('Conv') != -1:
                nn.init.normal_(m.weight.data, 0.0, 0.02)
            elif classname.find('BatchNorm') != -1:
                nn.init.normal_(m.weight.data, 1.0, 0.02)
                nn.init.constant_(m.bias.data, 0)
        
        netD.apply(weights_init)

        # Load the Generator from the model 
        if opt.model == '':
            print('------- This code assumes that you load a model of a Generator -------')
            exit(-1)
        else :
            netD.load_state_dict(arg_parser.checkpoint['netD'])
        number = len(os.listdir(opt.outf))

        # Set the model of the generator to evaluation
        netD.eval()

        input = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize).cuda()

        features = np.array([])
        labels = np.array([])

        # Genrate the feature vector for each pic in the data
        for i, data in enumerate(dataloader):
            images, label = data
            images = images.cuda()
            input.resize_as_(images).copy_(images)
            input_vector = Variable(input)

            feature = netD.get_features(input_vector)
            feature = feature.data.cpu().numpy()
            feature = feature.astype(np.float16)

            if features.size == 0:
                features = feature
                labels = label
            else:
                features = np.concatenate((features, feature), axis=0)
                labels = np.concatenate((labels, label), axis=0)
            if i % 5 == 0 and i :
                print('processed ', i)
                break
            if i == 50:
                break

        # split to train and validation sets
        indexes = list(range(len(labels)))
        print('number of samples ', labels.shape)
        print(features.shape)

        features = np.concatenate((features, labels[:, np.newaxis]), axis=1)

        features = features.astype(np.float16)

        np.savetxt(file, features)

    else:
        # if load features from file
        print('load features')
        data = np.loadtxt(file, dtype=np.float16)
        features, labels = data[:, : -1], data[:, -1: ]

        print('number of samples ', labels.shape)
        print(features.shape)

        indexes = list(range(len(labels)))
        np.random.shuffle(indexes)

        ratio = int(0.3 * len(labels))

        train_data, train_labels = features[indexes[: ratio]], labels[indexes[: ratio]]
        val_data, val_labels = features[indexes[ratio :]], labels[indexes[ratio :]]
        print('len train :', len(train_labels))
        print('len val: ', len(val_labels))

        if opt.svm == 'train':
            print('-------Train svm-------')
            clf = svm.SVC(decision_function_shape='ovo')
            clf.fit(train_data, train_labels)
            joblib.dump(clf, 'svm.pkl')
        elif opt.svm == 'validate':
            print('-------Download svm-------')
            clf = joblib.load('svm.pkl')
            print('-------Predict svm-------')
            val_labels = val_labels.squeeze()
            predicted_labels = clf.predict(val_data)
            print(len(val_labels), len(predicted_labels))
            a = predicted_labels == val_labels
            print(np.sum(a))
            accuracy = np.sum(predicted_labels == val_labels) / len(val_labels)
        
        print('-------SVM accuracy: %.4f accuracy-------'%accuracy )

        # precision and recall
        uniq_labels = set(val_labels)
        for label in uniq_labels:
            tp = np.sum(predicted_labels[predicted_labels == label] ==
                        val_labels[predicted_labels == label])
            fn_tp = np.sum(val_labels == label)
            fn = fn_tp - tp
            fp = np.sum(predicted_labels[predicted_labels == label] !=
                        val_labels[predicted_labels == label])

            precision = tp / (tp + fp)
            recall = tp / (tp + fn)
            print('class %d :: precision %.4f  recall %.4f '%(label, precision, recall))











