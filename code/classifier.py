from sklearn import svm
import os
import torch.utils.data
from torch.autograd import Variable
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.nn as nn
import dcgan
from arg_parser import opt
import arg_parser
import numpy as np
from sklearn.externals import joblib
from sklearn.preprocessing import RobustScaler

# python classifier_train.py --dataroot data/food-101/images --model results/models/model_12.pth --svm=train
# C:\Users\johan\OneDrive\Dokumente\Studium\ws1819\dl\project\synthesis_using_gans\code

transform = transforms.Compose([
                               transforms.Resize(opt.imageSize),
                               transforms.CenterCrop(opt.imageSize),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                               ])



if __name__ == '__main__':
    file = 'data/disc_features/features_food_101'
    
    if opt.featureGeneration :
            
        print('------- Save the features generated by the Discriminator -------')
        dataset = dset.ImageFolder(root=opt.dataroot,
                                    transform=transform)

        print('------- Load the data -------')
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,
                                                    shuffle=True, num_workers=int(opt.workers))
        
        print('------- Data successfully loaded -------')    
        device = torch.device("cuda:" + str(opt.gpu) if opt.cuda else "cpu")
            
        # Handle multi-gpu if desired
        netD = dcgan.Discriminator(opt.ngpu).to(device)

        if (device.type == 'cuda') and (opt.ngpu > 1):
            netD = nn.DataParallel(netD, list(range(opt.ngpu)))

        print('------- Load the discriminator -------')  
        # Load the Discriminator from the model 
        if opt.model == '':
            print('------- This code assumes that you load a model of a Generator -------')
            exit(-1)
        else :
            netD.load_state_dict(arg_parser.checkpoint['netD'])

        number = len(os.listdir(opt.outf))

        # Set the mode of the generator to evaluation
        netD.eval()

        input = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize).to(device)

        features = np.array([],dtype=np.float32)
        labels = np.array([])

        print('------- Start generating the features -------')  
        # Genrate the feature vector for each pic in the data and train the svm
        for i, data in enumerate(dataloader):

            # Load images and labels for batch from dataloader
            images, label = data
            # Move images to device
            images = images.to(device)

            '''
            Possibility number 1:
            # copy images into input vector
            input.resize_as_(images).copy_(images)
            input_vector = Variable(input).to(device)
            
            Now using possiblity number 2: 
            '''

            # Generate features with the Discriminator of the dcgan
            feature = netD.get_features(images)
            feature = feature.data.cpu().numpy()
            feature = feature.astype(np.float32)

            # Add features and labels to 
            if features.size == 0:
                features = feature
                labels = label
            else:
                features = np.concatenate((features, feature), axis=0)
                labels = np.concatenate((labels, label), axis=0)
            if i % 10 == 0 and i :
                print('[%d/%d]\t Progrss of feature generation'
                    % ( i, len(dataloader)))
                #break
    
    # store features an labels as one float vector
        '''
        Uncomment if you want to save features in file (Be careful it will be a very large file): 

        features = np.concatenate((features, labels[:, np.newaxis]), axis=1)
        features = features.astype(np.float16)
        np.savetxt(file, features)
        '''
        indexes = list(range(len(labels)))
        np.random.shuffle(indexes)    
        ratio = int(0.7 * len(labels))
        train_data, train_labels = features[indexes[: ratio]], labels[indexes[: ratio]]
        val_data, val_labels = features[indexes[ratio :]], labels[indexes[ratio :]]

        # Add a scaler for better processing of the svm

        scaler = RobustScaler()
        

        print('------- Successfully generated train and test data -------')
        print('len train :', len(train_labels))
        print('len val: ', len(val_labels))   


        if opt.svm == 'train':
            print('-------Train svm-------')
            train_data_rescaled = scaler.fit_transform(train_data)
            clf = svm.SVC(decision_function_shape='ovo', gamma='auto')
            clf.fit(train_data_rescaled, train_labels)
            joblib.dump(clf, 'svm_food-101_30_classes.pkl')
        elif opt.svm == 'validate':
            print('-------Download svm-------')
            clf = joblib.load('svm.pkl')
            print('-------Predict svm-------')
            val_labels = val_labels.squeeze()
            val_data_rescaled = scaler.fit_transform(val_data)
            predicted_labels = clf.predict(val_data_rescaled)
            print(len(val_labels), len(predicted_labels))
            a = predicted_labels == val_labels
            print(np.sum(a))
            accuracy = np.sum(predicted_labels == val_labels) / len(val_labels)
            
            print('-------SVM accuracy: %.4f accuracy-------'%accuracy )

            # precision and recall
            uniq_labels = set(val_labels)
            for label in uniq_labels:
                tp = np.sum(predicted_labels[predicted_labels == label] ==
                            val_labels[predicted_labels == label])
                fn_tp = np.sum(val_labels == label)
                fn = fn_tp - tp
                fp = np.sum(predicted_labels[predicted_labels == label] !=
                            val_labels[predicted_labels == label])

                precision = tp / (tp + fp)
                recall = tp / (tp + fn)
                print('class %d :: precision %.4f  recall %.4f '%(label, precision, recall))

    else : 

        print('------- Load features -------')
        data = np.loadtxt(file, dtype=np.float16)
        features, labels = data[:, : -1], data[:, -1: ]

        print('------- Features successfully loaded -------')
        indexes = list(range(len(labels)))
        np.random.shuffle(indexes)    
        ratio = int(0.7 * len(labels))
        train_data, train_labels = features[indexes[: ratio]], labels[indexes[: ratio]]
        val_data, val_labels = features[indexes[ratio :]], labels[indexes[ratio :]]

        print('------- Successfully generated train and test data -------')
        print('len train :', len(train_labels))
        print('len val: ', len(val_labels))   


        if opt.svm == 'train':
            print('-------Train svm-------')
            clf = svm.SVC(decision_function_shape='ovo', gamma='auto')
            clf.fit(train_data, train_labels)
            joblib.dump(clf, 'svm.pkl')
        elif opt.svm == 'validate':
            print('-------Download svm-------')
            clf = joblib.load('svm.pkl')
            print('-------Predict svm-------')
            val_labels = val_labels.squeeze()
            predicted_labels = clf.predict(val_data)
            print(len(val_labels), len(predicted_labels))
            a = predicted_labels == val_labels
            print(np.sum(a))
            accuracy = np.sum(predicted_labels == val_labels) / len(val_labels)
            
            print('-------SVM accuracy: %.4f accuracy-------'%accuracy )

            # precision and recall
            uniq_labels = set(val_labels)
            for label in uniq_labels:
                tp = np.sum(predicted_labels[predicted_labels == label] ==
                            val_labels[predicted_labels == label])
                fn_tp = np.sum(val_labels == label)
                fn = fn_tp - tp
                fp = np.sum(predicted_labels[predicted_labels == label] !=
                            val_labels[predicted_labels == label])

                precision = tp / (tp + fp)
                recall = tp / (tp + fn)
                print('class %d :: precision %.4f  recall %.4f '%(label, precision, recall))



            










